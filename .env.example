# ComfyUI Prompt Generator - Environment Configuration
# Copy this file to .env and customize the values for your setup

# Ollama Configuration
# URL for the Ollama API endpoint
OLLAMA_URL=http://localhost:11434/api/generate

# Default Ollama model to use for prompt generation
# Examples: qwen3:latest, llama2, mistral, codellama
OLLAMA_MODEL=qwen3:latest

# Flask Configuration
# Port for the Flask web server
FLASK_PORT=5000

# Debug mode (true for development, false for production)
FLASK_DEBUG=true

# Flask secret key for session management
# IMPORTANT: Generate a random secret key for production!
# You can generate one using: python -c "import secrets; print(secrets.token_hex(32))"
FLASK_SECRET_KEY=your-secret-key-here-change-this-in-production

# Logging Configuration
# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
# DEBUG: Detailed information, typically for debugging
# INFO: General informational messages
# WARNING: Warning messages for potentially harmful situations
# ERROR: Error messages for serious problems
# CRITICAL: Critical messages for very serious errors
LOG_LEVEL=INFO
