# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This is a **ComfyUI Prompt Generator** - a Flask web application that integrates with local Ollama to generate detailed prompts for ComfyUI image generation. It supports both Flux and SDXL models with dual operation modes (Quick Generate and Chat & Refine).

## Development Commands

### Quick Start with Make (Recommended)
```bash
# Complete setup
make install

# Configure environment
cp .env.example .env
# Edit .env with your settings

# Start the application
make run

# Run tests
make test

# Run linting
make lint

# View all available commands
make help
```

### Manual Setup (Alternative)
```bash
# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows

# Install dependencies
pip install -r requirements.txt

# For development (includes pytest, flake8)
pip install -r requirements-dev.txt

# Run the application
python prompt_generator.py
```

### Testing
```bash
# Run all tests
make test
# or: pytest

# Run tests with coverage
make test-cov
# or: pytest --cov=prompt_generator --cov-report=html

# Run specific test file
pytest tests/test_app.py
pytest tests/test_presets.py
```

### Linting
```bash
# Run flake8
make lint
# or: flake8 prompt_generator.py tests/

# Configuration in .flake8 (max line length: 120)
```

### Ollama Setup
```bash
# Show Ollama setup instructions
make setup-ollama

# Verify Ollama is running
curl http://localhost:11434

# List installed models
ollama list

# Install a model
ollama pull qwen3:latest
```

## Architecture

### Core Application Structure
- **Backend**: `prompt_generator.py` - Flask application (~1,760 lines)
- **Frontend**: `templates/index.html` - Single-page application with embedded CSS/JS and dark mode
- **Database**: SQLite (`prompt_history.db`) - Auto-created for prompt history
- **Dependencies**: Flask 3.0.0, requests 2.31.0, python-dotenv 1.0.0, sqlite3 (built-in)
- **Tests**: `tests/` directory with pytest fixtures in `conftest.py`

### Key Routes
- `GET /` - Serves the main application (`templates/index.html`)
- `GET /presets` - Returns available preset configurations (JSON)
- `POST /generate` - One-shot prompt generation (Quick Generate mode)
- `POST /generate-stream` - Streaming one-shot generation with Server-Sent Events
- `POST /chat` - Conversational prompt refinement (Chat & Refine mode)
- `POST /chat-stream` - Streaming conversational mode with real-time tokens
- `POST /reset` - Clears chat conversation history
- `GET /history` - Retrieve prompt generation history (supports search)
- `DELETE /history/<id>` - Delete a specific history item

### Error Handling
The application uses custom exception classes with specific HTTP error handlers:
- `OllamaConnectionError` → 503 Service Unavailable
- `OllamaTimeoutError` → 504 Gateway Timeout
- `OllamaModelNotFoundError` → 404 Not Found
- `OllamaAPIError` → 502 Bad Gateway

All routes return JSON error responses with `error`, `message`, `status`, and `type` fields.

### Session Management
- Chat mode uses Flask server-side sessions (signed cookies)
- Conversation history automatically limited to 20 messages + system prompt
- Sessions reset when switching models or calling `/reset`
- Trimming happens in `/chat` route when conversation exceeds 21 messages

### Preset System
The application includes 61 curated presets in the `PRESETS` dictionary (lines 391-480):
- **styles**: 14 options (Cinematic, Anime, Photorealistic, etc.)
- **artists**: 18 options (Greg Rutkowski, Ansel Adams, Studio Ghibli, etc.)
- **composition**: 15 options (Portrait, Landscape, Rule of Thirds, etc.)
- **lighting**: 15 options (Golden Hour, Neon, Volumetric, etc.)

Each preset is optional - all default to "None" with empty string value.

### Database System
The application uses SQLite to persist prompt generation history:
- **Database File**: `prompt_history.db` (auto-created in project root)
- **Schema**: See `init_db()` function (lines 145-179)
- **Table**: `prompt_history` with fields: id, timestamp, user_input, generated_output, model, presets, mode
- **Functions**: `save_to_history()`, `get_history()`, `delete_history_item()`
- Automatically saves all generations (oneshot and chat modes)
- Supports search queries across user input and output

### Model-Specific Prompting
Two distinct system prompts in `SYSTEM_PROMPTS` dictionary (lines 501-534):
- **Flux**: Natural language, conversational style, no quality tags needed
- **SDXL**: Quality-tagged prompts with separate negative prompt generation

### Streaming Support
The application supports real-time streaming responses via Server-Sent Events (SSE):
- **Endpoints**: `/generate-stream` and `/chat-stream`
- **Implementation**: See `_stream_ollama_response()` function (lines 623-750)
- Tokens stream to frontend as they're generated by Ollama
- Provides responsive, real-time feedback to users
- Uses generator functions to yield tokens incrementally

## Configuration

### Environment Variables (.env)
Configuration is managed via `.env` file (copy from `.env.example`):

```bash
# Ollama Configuration
OLLAMA_URL=http://localhost:11434/api/generate
OLLAMA_MODEL=qwen3:latest

# Flask Configuration
FLASK_PORT=5000
FLASK_DEBUG=true  # false for production
FLASK_SECRET_KEY=your-secret-key-here  # Generate with: python -c "import secrets; print(secrets.token_hex(32))"

# Logging
LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR, CRITICAL
```

All configuration values have fallback defaults if .env is not present.

### Changing Configuration
Prefer editing `.env` over modifying code. Key locations if manual editing is needed:
- Ollama URL: Line 30
- Ollama model: Line 31
- Flask port: Line 32
- Flask debug: Line 33
- Secret key: Line 34
- Log level: Line 35

## Logging System

The application includes comprehensive logging:
- **Location**: `logs/app.log` (created automatically)
- **Rotation**: 10MB max file size, keeps 5 backups
- **Format**: `%(asctime)s - %(name)s - %(levelname)s - %(message)s`
- **Setup**: `setup_logging()` function (lines 38-80)

View logs:
```bash
make logs          # View last 50 lines
make logs-follow   # Follow logs in real-time
```

## Development Guidelines

### Adding New Presets
Edit `PRESETS` dictionary in `prompt_generator.py` (lines 108-186):
```python
PRESETS = {
    "styles": {
        "Your New Style": "style description, tags, keywords",
        # ...
    },
    # ... other categories
}
```

Presets are automatically available via `/presets` endpoint and frontend dropdowns.

### Adding New Routes
Follow the RESTful pattern:
```python
@app.route('/new-endpoint', methods=['POST'])
def new_function():
    logger.info("Received /new-endpoint request")

    # Validate JSON
    if not request.json:
        return jsonify({'error': 'Invalid request', 'message': 'Request must contain JSON data'}), 400

    data = request.json
    # Handle request

    logger.info("Successfully processed request")
    return jsonify({'result': 'response'})
```

Error handling is automatic via registered error handlers.

### Adding New Model Types
1. Add system prompt to `SYSTEM_PROMPTS` dictionary (lines 188-222)
2. Update frontend model selector in `templates/index.html`
3. No other code changes needed - the system is model-agnostic

### Modifying UI
All frontend code is in `templates/index.html` (single file):
- Structure: HTML with semantic markup
- Styling: Embedded CSS with custom properties
- Logic: Vanilla JavaScript (ES6+)
- No build step or external dependencies

### Testing Best Practices
- Tests use `monkeypatch` to mock Ollama calls (see `tests/test_app.py`)
- Fixtures available: `flask_app`, `client`, `presets` (defined in `conftest.py`)
- All tests must pass before CI succeeds (GitHub Actions)
- Coverage report generated with `make test-cov`

## Troubleshooting

### "Failed to connect to server"
1. Verify Ollama is running: `ollama serve`
2. Check Ollama API: `curl http://localhost:11434`
3. Verify model installation: `ollama list`
4. Check OLLAMA_URL in `.env` or code (line 30)
5. Review logs: `make logs`

### Empty or Poor Quality Prompts
- Try different Ollama models (`OLLAMA_MODEL` in `.env`)
- Provide more detailed input descriptions
- Use presets to guide AI output
- Switch to Chat mode for iterative refinement
- Check logs for errors

### Port Conflicts
- Change `FLASK_PORT` in `.env` file
- Common alternatives: 8080, 3000, 8000
- Verify port availability: `lsof -i :5000` (Linux/Mac)

### Tests Failing
- Ensure virtual environment is activated
- Install dev dependencies: `pip install -r requirements-dev.txt`
- Check pytest output for specific failures
- Verify imports work: `python -c "from prompt_generator import app"`

### Linting Errors
- Configuration in `.flake8` file
- Max line length: 120 characters
- Excludes: venv, logs, build artifacts
- Auto-fix some issues: Consider using `black` or `autopep8`

## File Structure

```
.
├── prompt_generator.py      # Main Flask application (630 lines)
├── templates/
│   └── index.html          # Single-page frontend
├── tests/
│   ├── __init__.py
│   ├── conftest.py         # Pytest fixtures
│   ├── test_app.py         # Route and functionality tests
│   └── test_presets.py     # Preset validation tests
├── requirements.txt         # Production dependencies
├── requirements-dev.txt     # Development dependencies (pytest, flake8)
├── .env.example            # Environment configuration template
├── .flake8                 # Linting configuration
├── Makefile                # Development commands
├── README.md               # User-facing documentation
├── ARCHITECTURE.md         # Detailed technical architecture
└── logs/                   # Application logs (auto-created)
    └── app.log
```

## Privacy and Security

- All processing happens locally via Ollama
- No external API calls or data collection
- Session data stored server-side (signed cookies)
- Secret key from environment variable (generate unique for production)
- No authentication (designed for single-user local use)
- Input validation on all routes
- Comprehensive error handling prevents information leakage
